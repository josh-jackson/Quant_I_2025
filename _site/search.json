[
  {
    "objectID": "Basics_1.html#glm",
    "href": "Basics_1.html#glm",
    "title": "What are models?",
    "section": "GLM",
    "text": "GLM\n\nGeneral(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "Basics_1.html#what-do-we-mean-by-generalized",
    "href": "Basics_1.html#what-do-we-mean-by-generalized",
    "title": "What are models?",
    "section": "What do we mean by General(ized)?",
    "text": "What do we mean by General(ized)?\n\nIt is general in that it refers to a broad set of similar models that can applied to almost any context"
  },
  {
    "objectID": "Basics_1.html#what-do-we-mean-by-linear",
    "href": "Basics_1.html#what-do-we-mean-by-linear",
    "title": "What are models?",
    "section": "What do we mean by linear?",
    "text": "What do we mean by linear?\n\nWe try to understand our dependent variable (DV) via a linear combination predictor variables.\nA linear combination a way of combining things (variables) using scalar multiplication and addition"
  },
  {
    "objectID": "Basics_1.html#what-is-a-model",
    "href": "Basics_1.html#what-is-a-model",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?"
  },
  {
    "objectID": "Basics_1.html#what-is-a-model-1",
    "href": "Basics_1.html#what-is-a-model-1",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?\n\na representation of the world\na statistical model uses math to make predictions about the world"
  },
  {
    "objectID": "Basics_1.html#middle-school-math",
    "href": "Basics_1.html#middle-school-math",
    "title": "What are models?",
    "section": "Middle School Math",
    "text": "Middle School Math\n\\[ y = mx + b \\] - what is \\(y\\)?\n\nwhat is \\(m\\)?\nwhat is \\(x\\)?\nwhat is \\(b\\)?"
  },
  {
    "objectID": "Basics_1.html#lets-rewrite-this",
    "href": "Basics_1.html#lets-rewrite-this",
    "title": "What are models?",
    "section": "Let’s rewrite this",
    "text": "Let’s rewrite this\n\\[y = b_0 + b_{1}X\\]\n\nwhat is \\(y\\)?\nwhat is \\(b_0\\)?\nwhat is \\(b_1\\)?\nwhat is \\(X\\)?"
  },
  {
    "objectID": "Basics_1.html#are-models-always-right",
    "href": "Basics_1.html#are-models-always-right",
    "title": "What are models?",
    "section": "Are models always right?",
    "text": "Are models always right?"
  },
  {
    "objectID": "Basics_1.html#models-are-flawed",
    "href": "Basics_1.html#models-are-flawed",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?"
  },
  {
    "objectID": "Basics_1.html#models-are-flawed-1",
    "href": "Basics_1.html#models-are-flawed-1",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "Basics_1.html#models",
    "href": "Basics_1.html#models",
    "title": "What are models?",
    "section": "Models",
    "text": "Models\n\nWhat are the goals of modeling?\nWhat do you need in order to develop a model?"
  },
  {
    "objectID": "Basics_1.html#how-do-we-know-if-a-model-is-good",
    "href": "Basics_1.html#how-do-we-know-if-a-model-is-good",
    "title": "What are models?",
    "section": "How do we know if a model is good?",
    "text": "How do we know if a model is good?\n\nWhat makes it good?"
  },
  {
    "objectID": "Basics_1.html#how-will-we-use-models",
    "href": "Basics_1.html#how-will-we-use-models",
    "title": "What are models?",
    "section": "How will we use models?",
    "text": "How will we use models?\n\nThis semester, we will mainly focus on classic statistical tests\nEvery single one of these is a model\nWe will also focus on developing your intuition\nWhen you face new models, come back to these basics"
  },
  {
    "objectID": "Basics_1.html#how-can-we-visualize-data-to-make-sense-of-it",
    "href": "Basics_1.html#how-can-we-visualize-data-to-make-sense-of-it",
    "title": "What are models?",
    "section": "How can we visualize data to make sense of it?",
    "text": "How can we visualize data to make sense of it?\n\n\nCode\nlibrary(broom)\nset.seed(123)\nx.1 &lt;- rnorm(100, 0, 1)\ne.1 &lt;- rnorm(100, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\nd.1\n\n\n             x.1         y.1\n1   -0.560475647 -1.22907473\n2   -0.230177489  0.88716980\n3    1.558708314  0.86390582\n4    0.070508391 -0.15630558\n5    0.129287735 -1.33212888\n6    1.715064987  1.35323029\n7    0.460916206 -0.81630503\n8   -1.265061235 -3.53166755\n9   -0.686852852 -0.63822211\n10  -0.445661970  2.09287913\n11   1.224081797  0.02255106\n12   0.359813827  1.91382625\n13   0.400771451 -2.51534112\n14   0.110682716  0.44975156\n15  -0.555841135  1.23310178\n16   1.786913137  2.08510895\n17   0.497850478  0.98517015\n18  -1.966617157 -1.86305145\n19   0.701355902 -0.81366295\n20  -0.472791408 -1.80829286\n21  -1.067823706  0.14799016\n22  -0.217974915 -1.51483543\n23  -1.026004448 -1.04541733\n24  -0.728891229 -0.41307456\n25  -0.625039268  3.84395241\n26  -1.686693311 -1.73158112\n27   0.837787044  1.43155602\n28   0.153373118  0.74027691\n29  -1.138136937 -2.04968858\n30   1.253814921  1.04698203\n31   0.426464221  3.62365704\n32  -0.295071483  1.24071879\n33   0.895125661  1.07478496\n34   0.878133488  0.13797975\n35   0.821581082 -3.15462485\n36   0.688640254  3.14142657\n37   0.553917654 -2.11662543\n38  -0.061911711  1.94584358\n39  -0.305962664  4.14992767\n40  -0.380471001 -2.59704537\n41  -0.694706979  1.52147983\n42  -0.207917278 -0.13874948\n43  -1.265396352 -3.34025631\n44   2.168955965 -1.33640953\n45   1.207961998 -2.03869325\n46  -1.123108583 -1.17952277\n47  -0.402884835 -2.64509783\n48  -0.466655354  1.61917310\n49   0.779965118  5.12919870\n50  -0.083369066 -2.11991394\n51   0.253318514  2.21480288\n52  -0.028546755  2.02238377\n53  -0.042870457  1.14082641\n54   1.368602284 -0.76402196\n55  -0.225770986  0.13692074\n56   1.516470604  0.77326816\n57  -1.548752804  0.77416502\n58   0.584613750  0.07666005\n59   0.123854244  2.52206661\n60   0.215941569 -0.13039385\n61   0.379639483  2.81422465\n62  -0.502323453 -1.87463191\n63  -0.333207384 -2.20357455\n64  -1.018575383  6.42186341\n65  -1.071791226 -0.92320035\n66   0.303528641  1.26339594\n67   0.448209779  2.01965473\n68   0.053004227 -0.43840893\n69   0.922267468  2.04097120\n70   2.050084686  2.36547563\n71  -0.491031166 -0.20082816\n72  -2.309168876 -0.63945681\n73   1.005738524  0.98502168\n74  -0.709200763  4.36684338\n75  -0.688008616 -1.36107693\n76   1.025571370 -1.12792828\n77  -0.284773007  0.41895164\n78  -1.220717712  0.44956676\n79   0.181303480  1.47276387\n80  -0.138891362 -0.49312091\n81   0.005764186 -1.62348197\n82   0.385280401  3.23827457\n83  -0.370660032 -0.40316379\n84   0.644376549 -0.87661862\n85  -0.220486562 -0.09382675\n86   0.331781964  0.28812829\n87   1.096839013  3.32310204\n88   0.435181491  0.90882440\n89  -0.325931586  1.82884520\n90   1.148807618  0.13326016\n91   0.993503856  1.47531774\n92   0.548396960  0.15224650\n93   0.238731735  0.82046951\n94  -0.627906076 -1.63607506\n95   1.360652449 -1.37324422\n96  -0.600259587  4.16428400\n97   2.187332993  2.90445079\n98   1.532610626 -1.15960688\n99  -0.235700359 -0.85196703\n100 -1.026420900 -2.43549166"
  },
  {
    "objectID": "Basics_1.html#how-do-we-visualize-categorical-data",
    "href": "Basics_1.html#how-do-we-visualize-categorical-data",
    "title": "What are models?",
    "section": "How do we visualize categorical data?",
    "text": "How do we visualize categorical data?\nNominal/categorical data does not have any inherent numbers associated with it. Think control/tx, eye color, etc.\n\n\nCode\nset.seed(123)\ngroup &lt;- c(0, 1)\nx.2 &lt;- rep(group, times = 50)\ne.1 &lt;- rnorm(100, 0, 1)\ny.1 &lt;- .5 + .85 * x.2 + e.1\nd.2 &lt;- data.frame(x.2,y.1)\nm.2 &lt;- lm(y.1 ~ x.2, data = d.2)\nd2.f&lt;- augment(m.2)\nd.2\n\n\n    x.2          y.1\n1     0 -0.060475647\n2     1  1.119822511\n3     0  2.058708314\n4     1  1.420508391\n5     0  0.629287735\n6     1  3.065064987\n7     0  0.960916206\n8     1  0.084938765\n9     0 -0.186852852\n10    1  0.904338030\n11    0  1.724081797\n12    1  1.709813827\n13    0  0.900771451\n14    1  1.460682716\n15    0 -0.055841135\n16    1  3.136913137\n17    0  0.997850478\n18    1 -0.616617157\n19    0  1.201355902\n20    1  0.877208592\n21    0 -0.567823706\n22    1  1.132025085\n23    0 -0.526004448\n24    1  0.621108771\n25    0 -0.125039268\n26    1 -0.336693311\n27    0  1.337787044\n28    1  1.503373118\n29    0 -0.638136937\n30    1  2.603814921\n31    0  0.926464221\n32    1  1.054928517\n33    0  1.395125661\n34    1  2.228133488\n35    0  1.321581082\n36    1  2.038640254\n37    0  1.053917654\n38    1  1.288088289\n39    0  0.194037336\n40    1  0.969528999\n41    0 -0.194706979\n42    1  1.142082722\n43    0 -0.765396352\n44    1  3.518955965\n45    0  1.707961998\n46    1  0.226891417\n47    0  0.097115165\n48    1  0.883344646\n49    0  1.279965118\n50    1  1.266630934\n51    0  0.753318514\n52    1  1.321453245\n53    0  0.457129543\n54    1  2.718602284\n55    0  0.274229014\n56    1  2.866470604\n57    0 -1.048752804\n58    1  1.934613750\n59    0  0.623854244\n60    1  1.565941569\n61    0  0.879639483\n62    1  0.847676547\n63    0  0.166792616\n64    1  0.331424617\n65    0 -0.571791226\n66    1  1.653528641\n67    0  0.948209779\n68    1  1.403004227\n69    0  1.422267468\n70    1  3.400084686\n71    0  0.008968834\n72    1 -0.959168876\n73    0  1.505738524\n74    1  0.640799237\n75    0 -0.188008616\n76    1  2.375571370\n77    0  0.215226993\n78    1  0.129282288\n79    0  0.681303480\n80    1  1.211108638\n81    0  0.505764186\n82    1  1.735280401\n83    0  0.129339968\n84    1  1.994376549\n85    0  0.279513438\n86    1  1.681781964\n87    0  1.596839013\n88    1  1.785181491\n89    0  0.174068414\n90    1  2.498807618\n91    0  1.493503856\n92    1  1.898396960\n93    0  0.738731735\n94    1  0.722093924\n95    0  1.860652449\n96    1  0.749740413\n97    0  2.687332993\n98    1  2.882610626\n99    0  0.264299641\n100   1  0.323579100"
  },
  {
    "objectID": "Basics_1.html#what-do-these-visualizations-have-in-common",
    "href": "Basics_1.html#what-do-these-visualizations-have-in-common",
    "title": "What are models?",
    "section": "What do these visualizations have in common?",
    "text": "What do these visualizations have in common?\n\nLINES!\nMost of what we are going to do is represent the relationship between variables with lines (or planes or hyperplanes once we get into 2 or more variables)"
  },
  {
    "objectID": "Basics_1.html#thinking-in-terms-of-models",
    "href": "Basics_1.html#thinking-in-terms-of-models",
    "title": "What are models?",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nModels help us draw the lines\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (hence forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)\n\n\\(b_{1}\\) describes the strength of association i.e. the line!"
  },
  {
    "objectID": "Basics_1.html#regression-equation",
    "href": "Basics_1.html#regression-equation",
    "title": "What are models?",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[Y_i = b_{0} + b_{1}X_i +e_i\\]\n\n\\(Y_i \\sim Normal(\\mu, \\sigma)\\)\nThe DV, \\(Y\\) is assumed to be distributed as a Gaussian normal, made up of \\(Y_i\\), with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)"
  },
  {
    "objectID": "Basics_1.html#regression-terms",
    "href": "Basics_1.html#regression-terms",
    "title": "What are models?",
    "section": "Regression terms",
    "text": "Regression terms\n\nY / DV / Outcome / Response / Criterion\nX / IV / Predictor / Explanatory variable\nRegression coefficient (weight) / b / b* / \\(\\beta\\)\nIntercept \\(b_0\\) / \\(\\beta_{0}\\)\nError / Residuals \\(e\\)\nPredictions \\(\\hat{Y}\\)"
  },
  {
    "objectID": "Basics_1.html#regression-models",
    "href": "Basics_1.html#regression-models",
    "title": "What are models?",
    "section": "Regression models",
    "text": "Regression models\n\nThese models are a way to convey the relationship between two (or more) variables. They translate our hypotheses into math.\nWe can use these models to get information we may be interested in (e.g. means, SEs) and test hypotheses about the relationship among variables\n“All models are wrong but some are useful (and some are better than others)” - George Box"
  },
  {
    "objectID": "Basics_1.html#parts-of-the-model",
    "href": "Basics_1.html#parts-of-the-model",
    "title": "What are models?",
    "section": "Parts of the model",
    "text": "Parts of the model\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\] \\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\nEach individual has a unique Y value an X value and a residual/error term\n\nThe model only has a single \\(b_{0}\\) and \\(b_{1}\\) term. These are the regression parameters. \\(b_{0}\\) is the intercept and \\(b_{1}\\) quantifies the relationship between your model of the world and the DV."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quant 1",
    "section": "",
    "text": "Welcome to the first day of class! If you want to follow along with examples, head over to github for all of the code and data. https://github.com/josh-jackson/Quant_I_2025\ndfg"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Text books\n\nLSR readings can be found in the free, online textbook, Learning Statistics with R by Danielle Navarro.\nStatistical thinking for the 21st century. By Russell A. Poldrack Statistical thinking for the 21st century\n\n\n\nWeekly schedule\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nReadings\nHW Due\n\n\n\n\n1\n8/26\nModels\n\n\n\n\n-\n8/28\nDescribing data\n\n\n\n\n2\n9/2\nGLM basics\nLSR Ch 1 & 3\n\n\n\n-\n9/4\nProbability distributions\nLSR Ch 5 & 6\n\n\n\n3\n9/9\nProbability distributions\nLSR Ch 5 & 6\nHW 1 due\n\n\n-\n9/11\nEstimation, Likelihood & Loss\nLSR Ch 9\n\n\n\n4\n9/16\nEstimation, Likelihood & Loss\nLSR Ch 9\n\n\n\n-\n9/18\nSimple regressions (1 predictor and t-tests)\nLSR Ch 10\n\n\n\n5\n9/23\nExam 1\n\n\n\n\n-\n9/25\nStatistical inference part 1 (Sampling and NHST)\nLSR Ch 11\nHW 2 due\n\n\n6\n9/30\nStatistical inference part 2 (Estimation)\nLSR Ch 11\n\n\n\n-\n10/2\nStatistical inference part 3 (Model comparisons)\nLSR Ch 11\n\n\n\n7\n10/7\nNo Class - Fall Break!\n\n\n\n\n-\n10/9\nSimulation and prediction\n\n\n\n\n8\n10/14\nWhat can go wrong (Transparency and Open science)\nLSR Ch 12\n\n\n\n-\n10/16\nAssumptions\nLSR Ch 13\n\n\n\n9\n10/21\nPower\nLSR Ch 13\n\n\n\n-\n10/23\nVisualizing Data\nLSR Ch 13\n\n\n\n10\n10/28\nThreats to Validity\n\n\n\n\n-\n10/30\nExam 2\n\nHW 3 due\n\n\n11\n11/4\n3 or more means (ANOVA)\n\n\n\n\n-\n11/6\nMultiple comparisons & contrasts\n\n\n\n\n12\n11/11\nInteractions\nLSR Ch 16\n\n\n\n-\n11/13\nANCOVA\n\n\n\n\n13\n11/18\nWithin-subjects designs\nLSR Ch 16\n\n\n\n-\n11/20\nBinomial and logistic regression\n\n\n\n\n14\n11/25\nChi-Square test\n\n\n\n\n-\n11/27\nNo Class – Happy Thanksgiving!\n\n\n\n\n15\n12/2\nEstimation revisited\n\nHW 5 due\n\n\n-\n12/4\nExam 3\n\n\n\n\n\n\n\nOverview\nThis course is the first in a two-course sequence that introduces necessary skills and knowledge for carrying out statistical analyses in the social sciences. The course has an applied focus for psychological research, but theoretical details will be emphasized when they facilitate understanding of key concepts. In other words, we will strike a balance between learning how to do statistics and knowing why we are doing it a particular way. Mastering both will facilitate generalization to new problems and techniques. Topics to be covered this semester include theoretical probability distributions (e.g., binomial, \\(t\\), \\(F\\), \\(\\chi^2\\)), general linear model basics, inferences about population means, confidence intervals, the pros and cons of null hypothesis testing, power, and effect sizes, and ANOVA.\n\nTextbook\nWe will primarily be referring to chapters in Learning Statistics with R by Danielle Navarro. This textbook is available for free online. You may choose to purchase a paper copy if you wish, but it is not required. Additional readings assignments will be posted here.\n\n\nR and RStudio\nStudents must have the latest version of R, which can be downloaded here. It is strongly recommended that students also download the RStudio GUI, available here. Both are free.\n\n\nResources for R and RStudio\nWhile we will be covering the use of R and RStudio extensively, one of the key skills required to use R is the ability to find answers on the Internet. The R community (sometimes referred to as the useR or rstats community) tends to be friendly and helpful and enjoys solving R-related problems in their spare time. For that reason, many common questions or problems have been posted to spaces online and answered by smart people. Finding and deciphering those answers is the key skill you should seek to hone this year. It’s much more important than remembering function names.\nHere’s a resource page to get you going.\n\n\n\nGraded materials\nYour final grade is comprised of three components:\n\n5 Homework: 6% each, 30% total\nExams: 20% for the first exam, 25% for exams 2 and 3, 70% total\n\n\nHomework assignments\nHomework assignments are intended to gauge your ability to apply the topics covered in class to the practice of data analysis. Homework assignments are to be done using R and RMarkdown; completed assignments should be submitted to Canvas. Please submit BOTH your .Rmd and .html file. Please do not use PDF or Word documents. There will be 5 assignments in total, each worth 6% of your grade.\nHWs are due at their due date by 9:30am. Grading will go by the following rubric:\n\n6/6: Completed all parts of the assignment, and results/interpretations were mostly correct (with maybe a few minor errors along the way)\n5/6: Completed all parts of the assignment, and ~25% of the results/interpretations are incorrect\n4/6: Completed all parts of the assignment, however ~50% of the results/interpretations are incorrect\n4/6: Either parts of the assignment are missing but what is there is correct, OR all parts have been completed but there are substantial issues with the results/interpretation\n2/6: Parts of the assignment are missing, and there are substantial issues with the results/interpretation\n1/6: Nearly all of the assignment is missing, but something was turned in\n0/6: No assignment submitted\n\nYou may work with your peers on homework assignments, with the following caveats:\n\nYour code cannot be 100% identical to your peer’s code (trust me, we can tell). It’s one thing to work together and consult, but it’s another thing to copy someone’s assignment. If it is determined that someone copied an entire homework assignment, both parties will be given automatic zero.\nIf you work with someone, you must acknowledge them in the assignment itself.\n\n\n\nExams\nThere will be 3 exams. They will test both your theoretical understanding of concepts, as well as your applied understanding of the material. You will not be expected to open R and code during these exams, however you will be expected to understand R code that has been provided. As exams get closer, I will go over the procedures and policies with you before the exam. If you have a disability that will require additional time, please contact me and Disability Resources as soon as possible (contact information below). The first exam is worth 20% of your grade. Exams 2 and 3 are each worth 25% of your grade.\n\n\nFinal Grading Rubric\nAll assignments and examinations are mandatory. If you need to miss an exam you must get in touch with me as soon as possible. Rescheduling of exams will be considered on a case by case basis, but I am not required to approve a rescheduled exam.\n\\[\\begin{equation}\n\\begin{split}\n\n& \\text{93% &gt;=  A} \\\\\n& \\text{90% - 92.99% = A-} \\\\\n& \\text{87% - 89.99% = B+} \\\\\n& \\text{83% - 86.99% = B} \\\\\n& \\text{80% - 82.99% = B-} \\\\\n& \\text{77% - 79.99% = C+} \\\\\n\n\\end{split}\n\n\\begin{split}\n& \\text{73% - 76.99% = C} \\\\\n& \\text{70% - 72.99% = C-} \\\\\n& \\text{67% - 69.99% = D+} \\\\\n& \\text{63% - 66.99% = D} \\\\\n& \\text{60% - 62.99% = D-} \\\\\n&\\text{&lt; 50% = F}\n\n\\end{split}\n\\end{equation}\\]\n\n\nStaying Connected\n\nWe will have a dedicated class Slack work space (see upper right corner of website for link). You can post questions that everyone can see, or make use of direct messaging. Shelly or the AIs will respond to your questions on Slack (if your peers don’t respond first!). This is really great for one-off questions, or if you need some quick clarification on something. We will also have a dedicated channel for silly programming and stats memes, because humor is important when you’re learning a skill like coding, in which you’ll repeatedly fail. Slack is not required per se, but it is highly encouraged.\nWe will occasionally have group discussions. Please engage with those around you.\n\n\n\nBe Considerate and Kind\n\nIt is OK to not be OK. If you tell me you’re having trouble, I’m not going to judge you or think less of you. I hope you will do the same for me. I will work with you to make sure we have a reasonable plan in place should something come up. However, this does require you telling me “hey, I’m not OK”.\nYou are always welcome to come talk to me about things that you’re going through. If I can’t help you, I usually know someone who can help – or I can at least give you some resources and point you in the right direction.\nIf you are struggling or need extra help, please just ask. I promise I will work with you.\n\n\n\nAttendance & In-person Etiquette\n\nI expect all students to be in class if they are able (not sick, not at a conference etc.). The best way to succeed is to come to class. Your chosen profession requires mastery of these skills! Come to class!\n\nYou are adults. Come to class when you can. Let me know when you can’t (quick email or Slack is OK).\n\nIf you are not feeling well, don’t come to class!! You have access to the slides and you can ask a friend for notes.\nMost people will have electronics with them in class. Drinks should have a lid on them so that if they spill, we hopefully aren’t ruining yours or another person’s laptop/tablet/phone etc. Be careful!\n\n\n\nOnline Etiquette\nThere will be a lot of online communication with the instructor, the AI, and your peers. You are expected to maintain a polite and respectful tone in their online discourse. Some things to consider:\n\nAny communication shared privately may become public, so be mindful of what you share in discussion boards or chats. This is especially true for sharing any personal and/or identifying information about you or someone else. Do not share any passwords or divulge any personal information (yours or others) that can be used in a malicious manner (phone numbers, addresses etc.).\nHumor doesn’t always translate in an online forum. If you want to make a joke or a sarcastic remark, be 100% sure that it is clear you are joking.\nYour comments must be readable to everyone. Very common acronyms are OK (“lol” or “haha”). But please refrain from acronyms that are not as well-known (“fwiw” etc.).\nTreat your classmates, professor, and AIs with kindness and respect. Any indication of online harassment or bullying will not be tolerated and will be reported. This is especially pertinent when giving constructive feedback in code reviews.\nPlease avoid using ALL CAPS because it can be interpreted as yelling.\n\n\n\n\nPolicies & Resources\nUniversity Code of Conduct:\n\nAny student found guilty of academic misconduct, such as cheating, plagiarizing, forgery, or furnishing false information to a University official will be subject to consequences including failing the class, suspension from the University, or expulsion from the University. Frankly, you’re in graduate school, and the purpose of work is to create opportunities to learn and improve. Even if cheating helps you in the short term, you’ll quickly find yourself ill-prepared for the career you have chosen. If you find yourself tempted to cheat, please come speak to me about an extension and developing tools to improve your success.\nPlease see the official University Code of Conduct\n\nSpecial Accommodations:\n\nThis includes but is not limited to a learning, sensory, or physical disability or any other diagnosis that requires special accommodations and/or assistance with lectures, reading, written assignments, and/or exam taking\nContact Disability Resources at disabilityresources@wustl.edu or call 314-935-5970\nPlease also contact me as soon as possible if you need special accommodations. Once I have the Accommodation Letter from Disability Resources, we can discuss ways to modify the course experience for you.\n\nMental & Physical Health:\n\nHabif Health and Wellness Center, email HabifInfo@wustl.edu or call 314-935-6666\n\nWUSTL Police Department:\n\nFor an on campus emergency, call 314-935-5555\nFor an off campus emergency, call 911\n\nRelationship or Sexual Violence (including sexual harassment and stalking):\n\nContact a licensed RSVP counselor (confidential, with some limited information being shared as needed with the appropriate university administrator(s)) at rsvpcenter@wustl.edu or call 314-935-3445\nContact the university’s Title IX Director, Ms. Jessica Kennedy, at jwkennedy@wustl.edu or call 314-935-3118\n\nYou can always come to me! Period. However, if you come to me with any issues surrounding child abuse, suicidal tendencies, or sexual assault, sexual discrimination, sexual harassment, dating violence, domestic violence or stalking, I am required to report these to their appropriate administrators. Washington University faculty and administrators strive to maintain confidentiality, but some information may need to be disclosed when it is a matter of safety."
  },
  {
    "objectID": "Estimation_5.html#today",
    "href": "Estimation_5.html#today",
    "title": "Estimation",
    "section": "Today",
    "text": "Today\n\nHow do I find the regression line\nHow do I evaluate the regression line"
  },
  {
    "objectID": "Estimation_5.html#how-do-i-find-the-regression-line",
    "href": "Estimation_5.html#how-do-i-find-the-regression-line",
    "title": "Estimation",
    "section": "How do I find the regression line?",
    "text": "How do I find the regression line?\n\nFor any given scatter plot, you could propose an infinite number of regression lines to best describe the relationship\nThe likelihood function calculates a value for each of those potential lines and tells you which are most “likely”"
  },
  {
    "objectID": "Estimation_5.html#ordinary-least-squares",
    "href": "Estimation_5.html#ordinary-least-squares",
    "title": "Estimation",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares"
  },
  {
    "objectID": "Estimation_5.html#when-ols-doesnt-work",
    "href": "Estimation_5.html#when-ols-doesnt-work",
    "title": "Estimation",
    "section": "When OLS doesn’t work",
    "text": "When OLS doesn’t work\n\nOLS is"
  },
  {
    "objectID": "Estimation_5.html#log-likelihood",
    "href": "Estimation_5.html#log-likelihood",
    "title": "Estimation",
    "section": "Log Likelihood",
    "text": "Log Likelihood\n\nMaximizing the log-likelihood is the same as maximizing the likelihood, but it’s mathematically and computationally much easier.\nLikelihood of observing all your data points is the product of the likelihood of each individual point. Multiplying many small probs will lead to issues. So we log them to turn it into addition.\nFor likelihood, the data are treated as a given, and value of theta varies. L(θ | data )"
  },
  {
    "objectID": "Estimation_5.html#log-likelihood-example",
    "href": "Estimation_5.html#log-likelihood-example",
    "title": "Estimation",
    "section": "Log Likelihood example",
    "text": "Log Likelihood example\n\ncalculate a mean and SD"
  },
  {
    "objectID": "Estimation_5.html#log-likelihood-example-1",
    "href": "Estimation_5.html#log-likelihood-example-1",
    "title": "Estimation",
    "section": "Log Likelihood example",
    "text": "Log Likelihood example\n\ndefine what values we want to calculate log likelihood for\n\n\nlibrary(psychTools)\nlibrary(tidyr)\nlibrary(tidyverse)\ngalton.data &lt;- galton\ngrid &lt;-\n  crossing(mu = seq(from = 66, to = 69, length.out = 200), sigma = seq(from = 2, to = 3, length.out = 200))\ngrid\n\n# A tibble: 40,000 × 2\n      mu sigma\n   &lt;dbl&gt; &lt;dbl&gt;\n 1    66  2   \n 2    66  2.01\n 3    66  2.01\n 4    66  2.02\n 5    66  2.02\n 6    66  2.03\n 7    66  2.03\n 8    66  2.04\n 9    66  2.04\n10    66  2.05\n# ℹ 39,990 more rows"
  },
  {
    "objectID": "Estimation_5.html#log-likelihood-example-2",
    "href": "Estimation_5.html#log-likelihood-example-2",
    "title": "Estimation",
    "section": "Log Likelihood example",
    "text": "Log Likelihood example\n\nNeed to calcualte the log likelihood of our data (each child’s height) assuming each part of our grid is true.\nTake the first line of our grid (mu = 66, sigma = 2) so we can see how likely each child (all 928) by dnorm(height, 66, 2)\nWe sum the LLs across all 928 participants for each spot in the grid to get the average log-likelihood for each grid spot"
  },
  {
    "objectID": "Estimation_5.html#model-fit",
    "href": "Estimation_5.html#model-fit",
    "title": "Estimation",
    "section": "Model fit",
    "text": "Model fit\n\nThe way the world is = our model + error\nHow good is our model? Is it a good representation of reality? Does it “fit” the data well?\nNeed to go beyond asking if it is significant, because what does that mean? Remember, all models are wrong\nWe are going to make predictions and see if the predictions (based on our model) matches our data"
  },
  {
    "objectID": "Estimation_5.html#model-fit-1",
    "href": "Estimation_5.html#model-fit-1",
    "title": "Estimation",
    "section": "Model fit",
    "text": "Model fit\n\nOur model is a prediction machine.\nThey are created by simply plugging a persons Xs into the created model\nIf you have bs and have Xs you can create a prediction\n\n\\(\\hat{Y}_{i}\\) = 2.65064 + -0.48111* \\(X_{i}\\)"
  },
  {
    "objectID": "Estimation_5.html#model-fit-2",
    "href": "Estimation_5.html#model-fit-2",
    "title": "Estimation",
    "section": "Model fit",
    "text": "Model fit\n\nWe want our predictions to be close to our actual data for each person ( \\(Y_{i}\\) )\nThe difference between the actual data and our our prediction ( \\(Y_{i} - \\hat{Y}_{i} = e\\) ) is the residual, how far we are “off”. This tells us how good our fit is.\nYou can have the same estimates for two models but completely different fit."
  },
  {
    "objectID": "Estimation_5.html#model-fit-3",
    "href": "Estimation_5.html#model-fit-3",
    "title": "Estimation",
    "section": "Model fit",
    "text": "Model fit\n\nCan you point out the predictions?\n\n\n\nCode\ntwogroup_fun = function(nrep = 100, b0 = 6, b1 = -2, sigma = 1) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\n\ntwogroup_fun2 = function(nrep = 100, b0 = 6, b1 = -2, sigma = 2) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\nset.seed(16)\nlibrary(broom)\nlm1 &lt;- augment(twogroup_fun())\n\nset.seed(16)\nlm2 &lt;- augment(twogroup_fun2())\n\nplot1&lt;- ggplot(lm1) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\nplot2&lt;- ggplot(lm2) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\n\nlibrary(gridExtra)\n grid.arrange(plot1, plot2, ncol=2)"
  },
  {
    "objectID": "Estimation_5.html#same-plot-with-continuous",
    "href": "Estimation_5.html#same-plot-with-continuous",
    "title": "Estimation",
    "section": "Same plot with continuous",
    "text": "Same plot with continuous"
  },
  {
    "objectID": "Estimation_5.html#partitioning-variance",
    "href": "Estimation_5.html#partitioning-variance",
    "title": "Estimation",
    "section": "Partitioning variance",
    "text": "Partitioning variance\n\\[\\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} -\\bar{Y})^2 + \\sum(Y - \\hat{Y})^2\\]"
  },
  {
    "objectID": "Estimation_5.html#sigma",
    "href": "Estimation_5.html#sigma",
    "title": "Estimation",
    "section": "Sigma",
    "text": "Sigma"
  },
  {
    "objectID": "glm_3.html#thinking-in-terms-of-models",
    "href": "glm_3.html#thinking-in-terms-of-models",
    "title": "GLM basics",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "glm_3.html#how-can-we-visualize-data-to-make-sense-of-it",
    "href": "glm_3.html#how-can-we-visualize-data-to-make-sense-of-it",
    "title": "GLM basics",
    "section": "How can we visualize data to make sense of it?",
    "text": "How can we visualize data to make sense of it?\n\n\nCode\nlibrary(broom)\nlibrary(tidyverse)\nset.seed(123)\nx.1 &lt;- rnorm(100, 0, 1)\ne.1 &lt;- rnorm(100, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\nd.1\n\n\n             x.1         y.1\n1   -0.560475647 -1.22907473\n2   -0.230177489  0.88716980\n3    1.558708314  0.86390582\n4    0.070508391 -0.15630558\n5    0.129287735 -1.33212888\n6    1.715064987  1.35323029\n7    0.460916206 -0.81630503\n8   -1.265061235 -3.53166755\n9   -0.686852852 -0.63822211\n10  -0.445661970  2.09287913\n11   1.224081797  0.02255106\n12   0.359813827  1.91382625\n13   0.400771451 -2.51534112\n14   0.110682716  0.44975156\n15  -0.555841135  1.23310178\n16   1.786913137  2.08510895\n17   0.497850478  0.98517015\n18  -1.966617157 -1.86305145\n19   0.701355902 -0.81366295\n20  -0.472791408 -1.80829286\n21  -1.067823706  0.14799016\n22  -0.217974915 -1.51483543\n23  -1.026004448 -1.04541733\n24  -0.728891229 -0.41307456\n25  -0.625039268  3.84395241\n26  -1.686693311 -1.73158112\n27   0.837787044  1.43155602\n28   0.153373118  0.74027691\n29  -1.138136937 -2.04968858\n30   1.253814921  1.04698203\n31   0.426464221  3.62365704\n32  -0.295071483  1.24071879\n33   0.895125661  1.07478496\n34   0.878133488  0.13797975\n35   0.821581082 -3.15462485\n36   0.688640254  3.14142657\n37   0.553917654 -2.11662543\n38  -0.061911711  1.94584358\n39  -0.305962664  4.14992767\n40  -0.380471001 -2.59704537\n41  -0.694706979  1.52147983\n42  -0.207917278 -0.13874948\n43  -1.265396352 -3.34025631\n44   2.168955965 -1.33640953\n45   1.207961998 -2.03869325\n46  -1.123108583 -1.17952277\n47  -0.402884835 -2.64509783\n48  -0.466655354  1.61917310\n49   0.779965118  5.12919870\n50  -0.083369066 -2.11991394\n51   0.253318514  2.21480288\n52  -0.028546755  2.02238377\n53  -0.042870457  1.14082641\n54   1.368602284 -0.76402196\n55  -0.225770986  0.13692074\n56   1.516470604  0.77326816\n57  -1.548752804  0.77416502\n58   0.584613750  0.07666005\n59   0.123854244  2.52206661\n60   0.215941569 -0.13039385\n61   0.379639483  2.81422465\n62  -0.502323453 -1.87463191\n63  -0.333207384 -2.20357455\n64  -1.018575383  6.42186341\n65  -1.071791226 -0.92320035\n66   0.303528641  1.26339594\n67   0.448209779  2.01965473\n68   0.053004227 -0.43840893\n69   0.922267468  2.04097120\n70   2.050084686  2.36547563\n71  -0.491031166 -0.20082816\n72  -2.309168876 -0.63945681\n73   1.005738524  0.98502168\n74  -0.709200763  4.36684338\n75  -0.688008616 -1.36107693\n76   1.025571370 -1.12792828\n77  -0.284773007  0.41895164\n78  -1.220717712  0.44956676\n79   0.181303480  1.47276387\n80  -0.138891362 -0.49312091\n81   0.005764186 -1.62348197\n82   0.385280401  3.23827457\n83  -0.370660032 -0.40316379\n84   0.644376549 -0.87661862\n85  -0.220486562 -0.09382675\n86   0.331781964  0.28812829\n87   1.096839013  3.32310204\n88   0.435181491  0.90882440\n89  -0.325931586  1.82884520\n90   1.148807618  0.13326016\n91   0.993503856  1.47531774\n92   0.548396960  0.15224650\n93   0.238731735  0.82046951\n94  -0.627906076 -1.63607506\n95   1.360652449 -1.37324422\n96  -0.600259587  4.16428400\n97   2.187332993  2.90445079\n98   1.532610626 -1.15960688\n99  -0.235700359 -0.85196703\n100 -1.026420900 -2.43549166"
  },
  {
    "objectID": "glm_3.html#general-linear-model-glm",
    "href": "glm_3.html#general-linear-model-glm",
    "title": "GLM basics",
    "section": "General linear model (GLM)",
    "text": "General linear model (GLM)\n\nThis model (equation) can be very simple as in a treatment/control experiment\nIt can be very complex in terms of trying to understand something like academic achievement\nThe majority of our models fall under the umbrella of a general(ized) linear model (often referred to as regression models)\nModels imply our theory about how the data are generated (ie how the world works)"
  },
  {
    "objectID": "glm_3.html#parts-of-the-model",
    "href": "glm_3.html#parts-of-the-model",
    "title": "GLM basics",
    "section": "Parts of the model",
    "text": "Parts of the model\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\]\n\nEach individual has a unique Y value an X value and a residual/error term\n\nThe model only has a single \\(b_{0}\\) and \\(b_{1}\\) term. These are the regression parameters. \\(b_{0}\\) is the intercept and \\(b_{1}\\) quantifies the relationship between your model of the world and the DV."
  },
  {
    "objectID": "glm_3.html#in-r",
    "href": "glm_3.html#in-r",
    "title": "GLM basics",
    "section": "in R",
    "text": "in R\n\nfit &lt;- lm(Y ~ X, data)\n\n\nName regression object\nUse linear model function, lm\nVariable for your DV/Y\n~ is interpretted as regressed on\nVariable for your IV/X"
  },
  {
    "objectID": "glm_3.html#what-do-the-estimates-tell-us",
    "href": "glm_3.html#what-do-the-estimates-tell-us",
    "title": "GLM basics",
    "section": "What do the estimates tell us?",
    "text": "What do the estimates tell us?\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\]\n\ncoef(m.1)\n\n(Intercept)         x.1 \n  0.2943939   0.4450568"
  },
  {
    "objectID": "glm_3.html#how-to-interpret-regression-estimates",
    "href": "glm_3.html#how-to-interpret-regression-estimates",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nThe entire class will go over different ways to interpret these estimates/parameters/coefficients\nIntercept (b0) signifies the level of Y when your model IVs (Xs) are zero\nRegression (b1) signifies the difference for a one unit change in your X"
  },
  {
    "objectID": "glm_3.html#regression-coefficient",
    "href": "glm_3.html#regression-coefficient",
    "title": "GLM basics",
    "section": "Regression coefficient",
    "text": "Regression coefficient\n“For a one unit change in X, there is a b1 predicted change in Y.”\n\\[\\large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}\\] \\[\\large r_{xy} = \\frac{s_{xy}}{s_xs_y}\\]"
  },
  {
    "objectID": "glm_3.html#pop-quiz",
    "href": "glm_3.html#pop-quiz",
    "title": "GLM basics",
    "section": "pop quiz",
    "text": "pop quiz\nIf I flipped the x and y variables, would I get the same regression coefficient?"
  },
  {
    "objectID": "glm_3.html#pop-quiz-2",
    "href": "glm_3.html#pop-quiz-2",
    "title": "GLM basics",
    "section": "pop quiz #2",
    "text": "pop quiz #2\nWhat would the regression coefficient be if I standardized (z-scored) the variables?"
  },
  {
    "objectID": "glm_3.html#intercept",
    "href": "glm_3.html#intercept",
    "title": "GLM basics",
    "section": "Intercept",
    "text": "Intercept\n\nThe intercept serves to adjust for differences in means between x and y.\nWhat is the intercept in a standardized regression?\nWhat happens to the intercept if I add 5 to x?"
  },
  {
    "objectID": "glm_3.html#centroid",
    "href": "glm_3.html#centroid",
    "title": "GLM basics",
    "section": "Centroid",
    "text": "Centroid\n\nThe mean of X and the mean of Y must always pass through the regression line\nWe will revisit this later in the semester as we discuss evaluating our models. The mean will be our baseline “guess” for using X to predict Y."
  },
  {
    "objectID": "glm_3.html#predictions",
    "href": "glm_3.html#predictions",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nThe regression line is made up of predictions\nWe will often call them \\(\\hat{Y}\\)\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE)"
  },
  {
    "objectID": "glm_3.html#predictions-1",
    "href": "glm_3.html#predictions-1",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\\[\\hat{Y_i} = b_{0} + b_{1}X_i \\] - People who have the same IV, get the same DV. No error in our equation."
  },
  {
    "objectID": "glm_3.html#predictions-2",
    "href": "glm_3.html#predictions-2",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\n\\(\\hat{Y_i}\\) are also known as fitted values\n\n\nd1.f&lt;- augment(m.1)\nd1.f\n\n# A tibble: 100 × 8\n      y.1     x.1 .fitted .resid   .hat .sigma   .cooksd .std.resid\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 -1.23  -0.560   0.0450 -1.27  0.0151   1.95 0.00336      -0.661 \n 2  0.887 -0.230   0.192   0.695 0.0112   1.95 0.000738      0.360 \n 3  0.864  1.56    0.988  -0.124 0.0361   1.95 0.0000796    -0.0652\n 4 -0.156  0.0705  0.326  -0.482 0.0100   1.95 0.000315     -0.250 \n 5 -1.33   0.129   0.352  -1.68  0.0100   1.94 0.00385      -0.872 \n 6  1.35   1.72    1.06    0.296 0.0420   1.95 0.000530      0.156 \n 7 -0.816  0.461   0.500  -1.32  0.0117   1.95 0.00274      -0.682 \n 8 -3.53  -1.27   -0.269  -3.26  0.0323   1.92 0.0487       -1.71  \n 9 -0.638 -0.687  -0.0113 -0.627 0.0173   1.95 0.000935     -0.326 \n10  2.09  -0.446   0.0960  2.00  0.0135   1.94 0.00733       1.04  \n# ℹ 90 more rows"
  },
  {
    "objectID": "glm_3.html#residuals",
    "href": "glm_3.html#residuals",
    "title": "GLM basics",
    "section": "Residuals",
    "text": "Residuals\nThe residuals can be thought of as what is left over in Y that is not explained by our model. ie difference between actual and predicted\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\]\n\\[\\hat{Y_i} = b_{0} + b_{1}X_i\\] \\[{Y_i} - \\hat{Y_i} = e_i \\]"
  },
  {
    "objectID": "glm_3.html#residuals-1",
    "href": "glm_3.html#residuals-1",
    "title": "GLM basics",
    "section": "Residuals",
    "text": "Residuals\n\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE)"
  },
  {
    "objectID": "glm_3.html#residuals-2",
    "href": "glm_3.html#residuals-2",
    "title": "GLM basics",
    "section": "Residuals",
    "text": "Residuals\n\nAssociation between \\(\\hat{Y_i}\\) and e?\n\n\nd1.f |&gt; \nggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth(se = F, method = \"lm\")"
  },
  {
    "objectID": "glm_3.html#sigma",
    "href": "glm_3.html#sigma",
    "title": "GLM basics",
    "section": "Sigma",
    "text": "Sigma\n\nHow do we know whether we have a lot or a little residuals?\nSigma (σ) is the SD of residuals. It can be thought of as how much left over in Y that we cannot explain by our model."
  },
  {
    "objectID": "Distributions_4.html#today",
    "href": "Distributions_4.html#today",
    "title": "Probability Distributions",
    "section": "Today",
    "text": "Today\n\nChoosing the right regression model\nOften this boils down to how you measure your variables\nA model built on poor measures is like a statistical house on a cracked foundation, destined to crumble under the slightest scrutiny\nThese measurement choices are just the bare minimum for quality models and inferences"
  },
  {
    "objectID": "Distributions_4.html#data-generating-process-dgp",
    "href": "Distributions_4.html#data-generating-process-dgp",
    "title": "Probability Distributions",
    "section": "Data generating process (DGP)",
    "text": "Data generating process (DGP)\n\nFor every DV we are interested in, we have to think about what produces it (i.e. the rules)\nMost of the time we have no idea (this is why we are doing the experiment), but it is helpful to think about a simplified approximation of the true, unknown DGP\nThe goal of fitting a regression model is to use your sample data to estimate the parameters of a chosen probability distribution that represent this DGP."
  },
  {
    "objectID": "Distributions_4.html#what-are-probability-distributions",
    "href": "Distributions_4.html#what-are-probability-distributions",
    "title": "Probability Distributions",
    "section": "What are probability distributions?",
    "text": "What are probability distributions?\n\nIn stats: mathematical function that describes the likelihood of all possible outcomes for a random variable\nFor our purposes, we will use them to describe the assumed pattern of variability (distribution) in the outcome variable\nChoosing a probability distribution ~= choosing a DGP"
  },
  {
    "objectID": "Distributions_4.html#common-theoretical-probability-distributions",
    "href": "Distributions_4.html#common-theoretical-probability-distributions",
    "title": "Probability Distributions",
    "section": "Common theoretical probability distributions",
    "text": "Common theoretical probability distributions\n\nNormal Distribution\nt-Distribution\nF-Distribution\nBernoulli Distribution\nBinomial Distribution\nNegative Binomial\nChi-square distribution\nPoisson Distribution\nMultinomial (categorical)"
  },
  {
    "objectID": "Distributions_4.html#parameters",
    "href": "Distributions_4.html#parameters",
    "title": "Probability Distributions",
    "section": "Parameters",
    "text": "Parameters\n\nEach distribution comes with a set of parameters that are used to describe the distribution\nDescribes the likelihood of all possible outcomes for a random variable. E.g., coin flip. The Bernoulli distribution is the mathematical rule that describes the probability of heads or tails for a coin that has p probability of heads\nWe can represent these via a Probability Mass Function (PMF) for discrete variables or a Probability Density Function (PDF) for continuous variables."
  },
  {
    "objectID": "Distributions_4.html#probability-mass-function-pmf",
    "href": "Distributions_4.html#probability-mass-function-pmf",
    "title": "Probability Distributions",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\n\nFor discrete variables\nThe PMF gives you the probability of observing exactly a specific outcome\n\n\nlibrary(ggplot2)\n# Set the parameters\nn &lt;- 10 ;p &lt;- 0.5; x &lt;- 0:n\n# Calculate the probability mass function\npmf &lt;- dbinom(x, n, p)\npmf_df &lt;- data.frame(x, pmf)\n#creates a ggplot object with pmf_df as the data source \nggplot(pmf_df, aes(x = x, y = pmf)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"PMF of Binomial Distribution\") + xlab(\"Number of Successes\") +\n  ylab(\"Probability\") +scale_x_continuous(breaks = seq(0, 10, by = 1))"
  },
  {
    "objectID": "Distributions_4.html#probability-density-function-pdf",
    "href": "Distributions_4.html#probability-density-function-pdf",
    "title": "Probability Distributions",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\n\nFor continuous variables.\nThe value of the PDF at any specific point is not a probability. The probability of observing exactly one value is zero.\nProbability is represented by the area under the curve over a range (density)\n\n\nx &lt;- seq(from = -5, to = 5, by = 0.05)\nnorm_dat &lt;- data.frame(x = x, pdf = dnorm(x))\nggplot(norm_dat) + geom_line(aes(x = x, y = pdf)) +  ggtitle(\"PDF of Normal Distribution\") +\n  ylab(\"Density\")"
  },
  {
    "objectID": "Distributions_4.html#cumulative-distribtuion-function-cdf",
    "href": "Distributions_4.html#cumulative-distribtuion-function-cdf",
    "title": "Probability Distributions",
    "section": "Cumulative distribtuion function (CDF)",
    "text": "Cumulative distribtuion function (CDF)\n\nThe CDF applies to both discrete and continuous variables. It gives the “running total” of probability. The CDF at a value x is the probability of observing an outcome less than or equal to x.\nWill use to calculate p-values and percentiles or quantiles\n\n\n# Set the parameters\nn &lt;- 10;p &lt;- 0.5;x &lt;- 0:n\n# Cumulative Distribution Function\ncdf &lt;- pbinom(x, n, p)\ncdf_df &lt;- data.frame(x, cdf)\n#creates a ggplot object with cdf_df as the data source\nggplot(cdf_df, aes(x = x, y = cdf)) +\n  geom_bar(stat = \"identity\") + \n  ggtitle(\"CDF of Binomial Distribution\") +\n  xlab(\"Number of Successes\") + ylab(\"Cumulative Probability\")"
  },
  {
    "objectID": "Distributions_4.html#binomial-distribution",
    "href": "Distributions_4.html#binomial-distribution",
    "title": "Probability Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nTheoretical probability distribution appropriate when modeling the expected binary outcomes, X, across N trials, with the same probability of success (p) across independent trials.\nPMF: \\[P(X = x|n,p) = \\frac{N!}{X!(N-X)!}p^X(1-p)^{N-X}\\]\nX = Number of successes p = probability of success n = number of trials\nFirst part tells you the number of ways to select k success from a set of N, where order doesn’t matter. Second tells you the probability of any given sequence. By multiplying the number of ways an event can happen by the probability of any single one of those ways, you get the total overall probability."
  },
  {
    "objectID": "Distributions_4.html#history-of-binomial",
    "href": "Distributions_4.html#history-of-binomial",
    "title": "Probability Distributions",
    "section": "History of Binomial",
    "text": "History of Binomial\n\nThe foundation of probability theory based ways to win gambling! (“problem of points”)\nBuilt upon Pascals’s triangle, which was a way to describe the binomial coefficient (the number of ways k successes can occur in n trials)\n\nPMF: \\[\\frac{N!}{X!(N-X)!} = \\binom{N}{X}\\]\n\nLaid the groundwork for setting up expected values \\(E(x)\\) which are long run averages\nJacob Bernoulli took Pascal’s binomial coefficient and ran with it, using it to set up the Law of Large Numbers (more later)"
  },
  {
    "objectID": "Distributions_4.html#binomial-distribution-1",
    "href": "Distributions_4.html#binomial-distribution-1",
    "title": "Probability Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nOur mean and variance are related in the binomial distribution\n\nE(X) = Np\nVar(X) = Np(1-p)"
  },
  {
    "objectID": "Distributions_4.html#binomial-distribution-2",
    "href": "Distributions_4.html#binomial-distribution-2",
    "title": "Probability Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nWhat proportion of outcomes in the sample space that fall at or below a given outcome, say 75%?\n\nn &lt;- 20 ;p &lt;- 0.5; x &lt;- 0:n\n# Calculate the probability mass function\npmf &lt;- dbinom(x, n, p)\npmf_df &lt;- data.frame(x, pmf)\n#creates a ggplot object with pmf_df as the data source \nggplot(pmf_df, aes(x = x, y = pmf)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"PMF of Binomial Distribution, p = .5, N = 20\") + xlab(\"Number of Successes\") +\n  ylab(\"Probability\") +scale_x_continuous(breaks = seq(0, 20, by = 1))"
  },
  {
    "objectID": "Distributions_4.html#why-do-we-care",
    "href": "Distributions_4.html#why-do-we-care",
    "title": "Probability Distributions",
    "section": "Why do we care?",
    "text": "Why do we care?\n\\[X \\sim \\text{Bin}(n ,p)\\]\nThe binomial is of interest beyond describing the behavior of dice and coins. For example, suppose I give a 40-item multiple choice test, with each question having 4 options.\nI am worried that students might do well by chance alone. I would not want to pass students in the class if they were just showing up for the exams and guessing for each question.\nWhat are the parameters in the binomial distribution that will help me address this question?"
  },
  {
    "objectID": "Distributions_4.html#bernoulli-distribution",
    "href": "Distributions_4.html#bernoulli-distribution",
    "title": "Probability Distributions",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\n\\[X \\sim \\text{Bin}(n = 1,p)\\]\n\\[X \\sim \\text{Bern}(p)\\]\n\nWe will use this theoretical probability distribution when our DV comes in 1/0 or yes/no. This is a logistic regression."
  },
  {
    "objectID": "Distributions_4.html#poisson",
    "href": "Distributions_4.html#poisson",
    "title": "Probability Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nWhat if you have not single success, but counts of successes?\nCan be thought of as a binomial that has infinite trials (when p is small).\nCan think of this as the average “rate” often across time or space. E.g., Number of donkey deaths per day.\n\n\\[X \\sim \\text{Pois}(\\lambda)\\]\n\\[P(X=x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\]"
  },
  {
    "objectID": "Distributions_4.html#history-of-poisson",
    "href": "Distributions_4.html#history-of-poisson",
    "title": "Probability Distributions",
    "section": "History of Poisson",
    "text": "History of Poisson\n\nWrongful convictions and donkey deaths\nNumber of soldiers killed by being kicked by a horse each year in each of 14 cavalry corps over a 20-year period"
  },
  {
    "objectID": "Distributions_4.html#negative-binomial-distribution",
    "href": "Distributions_4.html#negative-binomial-distribution",
    "title": "Probability Distributions",
    "section": "Negative binomial distribution",
    "text": "Negative binomial distribution\n\nThe poisson mean must be equal to its variance. Often with counts we get means that are much less than the variance. This is called overdispersion\nOverdispersion can happen when the rate is not constant. If soldiers get kicked in the head more on weekends than week days, there will be more variability, but it might not influence the mean.\n\n\\[X \\sim \\text{NB}(\\mu, \\theta)\\]\n\\[\\text{Var}(X) = \\mu + \\frac{\\mu^2}{\\theta}\\]"
  },
  {
    "objectID": "Distributions_4.html#negative-binomial-distribution-1",
    "href": "Distributions_4.html#negative-binomial-distribution-1",
    "title": "Probability Distributions",
    "section": "Negative binomial distribution",
    "text": "Negative binomial distribution\n\nIt is actually a mixture between the poisson and another distribution, gamma. Sometimes called Gamma Poisson. It is a mixture of two theoretical probability distributions, which you will learn about later.\n\n\\[P(X=x) = \\frac{\\Gamma(x+\\theta)}{\\Gamma(x+1)\\Gamma(\\theta)} \\left(\\frac{\\theta}{\\mu+\\theta}\\right)^\\theta \\left(\\frac{\\mu}{\\mu+\\theta}\\right)^x\\]"
  },
  {
    "objectID": "Distributions_4.html#normal-gaussian-distribution",
    "href": "Distributions_4.html#normal-gaussian-distribution",
    "title": "Probability Distributions",
    "section": "Normal (Gaussian) distribution",
    "text": "Normal (Gaussian) distribution\n\nx &lt;- seq(from = 30, to = 180, by = 0.05)\nnorm_dat &lt;- data.frame(x = x, pdf = dnorm(x, 100,15))\nggplot(norm_dat) + geom_line(aes(x = x, y = pdf)) +  ggtitle(\"PDF of Normal Distribution (100,15)\") +\n  ylab(\"Density\")"
  },
  {
    "objectID": "Distributions_4.html#normal-gaussian-distribution-1",
    "href": "Distributions_4.html#normal-gaussian-distribution-1",
    "title": "Probability Distributions",
    "section": "Normal (Gaussian) distribution",
    "text": "Normal (Gaussian) distribution\n\\[X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\]\n\\[f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\]"
  },
  {
    "objectID": "Distributions_4.html#standardized-normal",
    "href": "Distributions_4.html#standardized-normal",
    "title": "Probability Distributions",
    "section": "Standardized normal",
    "text": "Standardized normal\n\naka z-distribution as mean = 0, SD = 1\n\n\n# 1. Load the ggplot2 library\nlibrary(ggplot2)\n\n# 2. Set the parameters for the normal distribution\nmean_val &lt;- 0\nsd_val &lt;- 1\n\n# 3. Generate a sequence of x-values for the curve\nx_values &lt;- seq(mean_val - 4 * sd_val, mean_val + 4 * sd_val, length.out = 500)\n\n# 4. Calculate the probability density for each x-value\ny_values &lt;- dnorm(x_values, mean = mean_val, sd = sd_val)\n\n# 5. Create a dataframe for plotting\nnormal_df &lt;- data.frame(\n  x = x_values,\n  density = y_values\n)\n\n# 6. Create the plot with improved labels\nggplot(normal_df, aes(x = x, y = density)) +\n\n  # --- Shaded Regions for the Empirical Rule ---\n  geom_ribbon(data = subset(normal_df, x &gt; mean_val - 3*sd_val & x &lt; mean_val + 3*sd_val),\n              aes(ymin = 0, ymax = density), fill = \"lightblue\", alpha = 0.8) +\n  geom_ribbon(data = subset(normal_df, x &gt; mean_val - 2*sd_val & x &lt; mean_val + 2*sd_val),\n              aes(ymin = 0, ymax = density), fill = \"royalblue\", alpha = 0.8) +\n  geom_ribbon(data = subset(normal_df, x &gt; mean_val - 1*sd_val & x &lt; mean_val + 1*sd_val),\n              aes(ymin = 0, ymax = density), fill = \"navyblue\", alpha = 0.8) +\n\n  # --- The Curve and Mean Line ---\n  geom_line(color = \"black\", size = 1) +\n  geom_vline(xintercept = mean_val, color = \"black\", linetype = \"dashed\", size = 1) +\n  \n  # --- Text Annotations (Revised) ---\n  # Label for 68% remains inside\n  annotate(\"text\", x = mean_val, y = dnorm(mean_val, mean_val, sd_val) * 0.5, \n           label = \"68.26% within 1 SD\", size = 5, color = \"white\", fontface = \"bold\") +\n           \n  # Label for 95% is moved above the curve\n  annotate(\"text\", x = mean_val + 1.6 * sd_val, y = dnorm(mean_val, mean_val, sd_val) * 0.9, \n           label = \"95.44% within 2 SD\", size = 4.5, color = \"black\") +\n           \n  # Arrow for 95% label\n  geom_segment(\n    aes(x = mean_val + 1.4 * sd_val, y = dnorm(mean_val, mean_val, sd_val) * 0.8,\n        xend = mean_val + 1.1 * sd_val, yend = dnorm(mean_val, mean_val, sd_val) * 0.4),\n    arrow = arrow(length = unit(0.2, \"cm\"))\n  ) +\n           \n  # Label for 99.7% is moved above the curve\n  annotate(\"text\", x = mean_val + 2.7 * sd_val, y = dnorm(mean_val, mean_val, sd_val) * 0.4, \n           label = \"99.7% within 3 SD\", size = 4.5, color = \"black\") +\n\n  # Arrow for 99.7% label\n  geom_segment(\n    aes(x = mean_val + 2.5 * sd_val, y = dnorm(mean_val, mean_val, sd_val) * 0.35,\n        xend = mean_val + 2.2 * sd_val, yend = dnorm(mean_val + 1.5*sd_val, mean_val, sd_val) * 0.2),\n    arrow = arrow(length = unit(0.2, \"cm\"))\n  ) +\n\n  # --- Labels and Theme ---\n  labs(\n    title = \"Visualizing the 68 / 95.44 / 99.7\",\n    x = \"Value\",\n    y = \"Probability Density\"\n  ) +\n  theme_minimal(base_size = 14) +\n  # Remove the legend for the fill colors\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Distributions_4.html#t-distribution",
    "href": "Distributions_4.html#t-distribution",
    "title": "Probability Distributions",
    "section": "t-distribution",
    "text": "t-distribution\n\\[X \\sim \\mathcal{t}(\\nu)\\] \\[X \\sim \\mathcal{t}(\\nu, \\mu, \\sigma^2)\\] - New parameter, nu\n\nLike the normal, but with larger tails.\nIn small samples, one finds more extreme scores than expected from if we rnorm some population parameters."
  },
  {
    "objectID": "Distributions_4.html#f-distribution",
    "href": "Distributions_4.html#f-distribution",
    "title": "Probability Distributions",
    "section": "F-distribution",
    "text": "F-distribution\n\\[X \\sim F(\\nu_1, \\nu_2)\\]"
  },
  {
    "objectID": "Distributions_4.html#chi-square-distribution",
    "href": "Distributions_4.html#chi-square-distribution",
    "title": "Probability Distributions",
    "section": "Chi-square distribution",
    "text": "Chi-square distribution\nTake a set of independent random variables that all come from a standard normal distribution, square each of them, and then add them all up, the resulting sum follows a chi-square distribution\n\\[X \\sim \\chi^2(k)\\] \\[ \\text{F distribution} = \\frac{\\chi_{1}^2/df1}{\\chi_{2}^2/{df1}}\\]"
  },
  {
    "objectID": "Distributions_4.html#chi-square-distirbution",
    "href": "Distributions_4.html#chi-square-distirbution",
    "title": "Probability Distributions",
    "section": "Chi square distirbution",
    "text": "Chi square distirbution\n\nAlso is used a lot in this class:\nGeneral/more applicable test for model comparisons\nChi-square test of association (categorical correlation)\nGoodness of fit (observed counts differ from expected)"
  }
]